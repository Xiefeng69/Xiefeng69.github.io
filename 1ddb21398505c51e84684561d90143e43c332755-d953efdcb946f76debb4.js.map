{"version":3,"file":"1ddb21398505c51e84684561d90143e43c332755-d953efdcb946f76debb4.js","mappings":"sKAAO,I,UCCIA,EAAiB,CAC1BC,WAAOC,EACPC,UAAMD,EACNE,eAAWF,EACXG,WAAOH,EACPI,UAAMJ,GAEGK,EAAc,iBAAuB,gBAAoBP,GCRhEQ,EAAoC,WAatC,OAZAA,EAAWC,OAAOC,QAAU,SAAUC,GACpC,IAAK,IAAIC,EAAGC,EAAI,EAAGC,EAAIC,UAAUC,OAAQH,EAAIC,EAAGD,IAG9C,IAAK,IAAII,KAFTL,EAAIG,UAAUF,GAGRJ,OAAOS,UAAUC,eAAeC,KAAKR,EAAGK,KAAIN,EAAEM,GAAKL,EAAEK,IAI7D,OAAON,GAGFH,EAASa,MAAMC,KAAMP,YAG1BQ,EAAgC,SAAUX,EAAGY,GAC/C,IAAIb,EAAI,GAER,IAAK,IAAIM,KAAKL,EACRH,OAAOS,UAAUC,eAAeC,KAAKR,EAAGK,IAAMO,EAAEC,QAAQR,GAAK,IAAGN,EAAEM,GAAKL,EAAEK,IAG/E,GAAS,MAALL,GAAqD,mBAAjCH,OAAOiB,sBAA2C,KAAIb,EAAI,EAAb,IAAgBI,EAAIR,OAAOiB,sBAAsBd,GAAIC,EAAII,EAAED,OAAQH,IAClIW,EAAEC,QAAQR,EAAEJ,IAAM,GAAKJ,OAAOS,UAAUS,qBAAqBP,KAAKR,EAAGK,EAAEJ,MAAKF,EAAEM,EAAEJ,IAAMD,EAAEK,EAAEJ,KAEhG,OAAOF,GAMT,SAASiB,EAAaC,GACpB,OAAOA,GAAQA,EAAKC,KAAI,SAAUC,EAAMlB,GACtC,OAAO,gBAAoBkB,EAAKC,IAAKxB,EAAS,CAC5CyB,IAAKpB,GACJkB,EAAKzB,MAAOsB,EAAaG,EAAKG,WAI9B,SAASC,EAAQC,GAEtB,OAAO,SAAUC,GACf,OAAO,gBAAoBC,EAAU9B,EAAS,CAC5CF,KAAME,EAAS,GAAI4B,EAAK9B,OACvB+B,GAAQT,EAAaQ,EAAKF,SAG1B,SAASI,EAASD,GACvB,IAAIE,EAAO,SAAcC,GACvB,IAMIpC,EANAE,EAAO+B,EAAM/B,KACbH,EAAOkC,EAAMlC,KACbsC,EAAQJ,EAAMI,MACdC,EAAWnB,EAAOc,EAAO,CAAC,OAAQ,OAAQ,UAE1CM,EAAexC,GAAQqC,EAAKrC,MAAQ,MAIxC,OAFIqC,EAAKpC,YAAWA,EAAYoC,EAAKpC,WACjCiC,EAAMjC,YAAWA,GAAaA,EAAYA,EAAY,IAAM,IAAMiC,EAAMjC,WACrE,gBAAoB,MAAOI,EAAS,CACzCoC,OAAQ,eACRC,KAAM,eACNC,YAAa,KACZN,EAAKlC,KAAMA,EAAMoC,EAAU,CAC5BtC,UAAWA,EACXC,MAAOG,EAASA,EAAS,CACvBP,MAAOoC,EAAMpC,OAASuC,EAAKvC,OAC1BuC,EAAKnC,OAAQgC,EAAMhC,OACtB0C,OAAQJ,EACRK,MAAOL,EACPM,MAAO,+BACLR,GAAS,gBAAoB,QAAS,KAAMA,GAAQJ,EAAMa,WAGhE,YAAuBhD,IAAhBK,EAA4B,gBAAoBA,EAAY4C,SAAU,MAAM,SAAUX,GAC3F,OAAOD,EAAKC,MACTD,EAAKvC,K,mBC4FZ,IAxK2B,SAACoD,GAExB,IAAMC,EAAW,CACb,CACIC,OAAQ,2HACRb,MAAO,qHACPc,OAAQ,gFACRC,KAAM,cACNC,KAAM,QACNC,KAAM,MACNC,IAAK,GACLC,KAAM,GACNC,KAAM,GACNC,MAAO,GACPC,QAAS,GACTC,SAAU,QACVC,KAAM,IAEV,CACIX,OAAQ,uHACRb,MAAO,iEACPc,OAAQ,0EACRC,KAAM,aACNC,KAAM,QACNC,KAAM,KACNC,IAAK,mDACLC,KAAM,sCACNC,KAAM,GACNC,MAAO,GACPC,QAAS,GACTC,SAAU,OACVC,KAAM,4pCAEV,CACIX,OAAQ,4GACRb,MAAO,6JACPc,OAAQ,yBACRC,KAAM,GACNC,KAAM,QACNC,KAAM,MACNC,IAAK,+DACLC,KAAM,GACNC,KAAM,GACNC,MAAO,GACPC,QAAS,GACTC,SAAU,QACVC,KAAM,y2DAEV,CACIX,OAAQ,+FACRb,MAAO,mHACPc,OAAQ,wCACRC,KAAM,GACNC,KAAM,QACNC,KAAM,GACNC,IAAK,2CACLC,KAAM,GACNC,KAAM,GACNC,MAAO,GACPC,QAAS,GACTC,SAAU,QACVC,KAAM,olCAEV,CACIX,OAAQ,qFACRb,MAAO,qEACPc,OAAQ,0EACRC,KAAM,aACNC,KAAM,QACNC,KAAM,KACNC,IAAK,gEACLC,KAAM,oCACNC,KAAM,GACNC,MAAO,gCACPC,QAAS,GACTC,SAAU,OACVC,KAAM,kuCAEV,CACIX,OAAQ,2IACRb,MAAO,oFACPc,OAAQ,kFACRC,KAAM,cACNC,KAAM,QACNC,KAAM,MACNC,IAAK,iEACLO,OAAQ,4BACRN,KAAM,oFACNC,KAAM,GACNC,MAAO,GACPC,QAAS,GACTC,SAAU,OACVC,KAAM,m4CAEV,CACIX,OAAQ,8GACRb,MAAO,sDACPc,OAAQ,iBACRC,KAAM,GACNC,KAAM,SACNC,KAAM,MACNC,IAAK,6CACLC,KAAM,GACNC,KAAM,GACNC,MAAO,GACPC,QAAS,wCACTC,SAAU,QACVC,KAAM,skDAEV,CACIX,OAAQ,+GACRb,MAAO,uEACPc,OAAQ,+CACRC,KAAM,WACNC,KAAM,QACNC,KAAM,MACNC,IAAK,gDACLC,KAAM,GACNC,KAAM,GACNC,MAAO,GACPC,QAAS,GACTC,SAAU,QACVC,KAAM,i/BAEV,CACIX,OAAQ,gGACRb,MAAO,qGACPc,OAAQ,8GACRC,KAAM,iBACNC,KAAM,QACNC,KAAM,MACNC,IAAK,iEACLC,KAAM,sCACNC,KAAM,UACNC,MAAO,kCACPC,QAAS,GACTC,SAAU,OACVC,KAAM,ykCAEV,CACIX,OAAQ,oGACRb,MAAO,oFACPc,OAAQ,oFACRC,KAAM,YACNC,KAAM,QACNC,KAAM,MACNC,IAAK,0CACLC,KAAM,sCACNC,KAAM,UACNC,MAAO,GACPC,QAAS,GACTC,SAAU,OACVC,KAAM,oxCAGd,GAAa,OAATb,EACA,OAAOC,EAGX,IADA,IAAIc,EAAe,GACnB,MAAmBd,EAAnB,eAA4B,CAAvB,IAAMe,EAAI,KACRA,EAAKV,MAAQN,GACZe,EAAaE,KAAKD,GAI1B,OAAOD","sources":["webpack://gatsby-starter-default/./node_modules/react-icons/lib/esm/iconsManifest.js","webpack://gatsby-starter-default/./node_modules/react-icons/lib/esm/iconContext.js","webpack://gatsby-starter-default/./node_modules/react-icons/lib/esm/iconBase.js","webpack://gatsby-starter-default/./src/utils/getPublication.js"],"sourcesContent":["export var IconsManifest = [{\n  \"id\": \"ci\",\n  \"name\": \"Circum Icons\",\n  \"projectUrl\": \"https://circumicons.com/\",\n  \"license\": \"MPL-2.0 license\",\n  \"licenseUrl\": \"https://github.com/Klarr-Agency/Circum-Icons/blob/main/LICENSE\"\n}, {\n  \"id\": \"fa\",\n  \"name\": \"Font Awesome 5\",\n  \"projectUrl\": \"https://fontawesome.com/\",\n  \"license\": \"CC BY 4.0 License\",\n  \"licenseUrl\": \"https://creativecommons.org/licenses/by/4.0/\"\n}, {\n  \"id\": \"fa6\",\n  \"name\": \"Font Awesome 6\",\n  \"projectUrl\": \"https://fontawesome.com/\",\n  \"license\": \"CC BY 4.0 License\",\n  \"licenseUrl\": \"https://creativecommons.org/licenses/by/4.0/\"\n}, {\n  \"id\": \"io\",\n  \"name\": \"Ionicons 4\",\n  \"projectUrl\": \"https://ionicons.com/\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://github.com/ionic-team/ionicons/blob/master/LICENSE\"\n}, {\n  \"id\": \"io5\",\n  \"name\": \"Ionicons 5\",\n  \"projectUrl\": \"https://ionicons.com/\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://github.com/ionic-team/ionicons/blob/master/LICENSE\"\n}, {\n  \"id\": \"md\",\n  \"name\": \"Material Design icons\",\n  \"projectUrl\": \"http://google.github.io/material-design-icons/\",\n  \"license\": \"Apache License Version 2.0\",\n  \"licenseUrl\": \"https://github.com/google/material-design-icons/blob/master/LICENSE\"\n}, {\n  \"id\": \"ti\",\n  \"name\": \"Typicons\",\n  \"projectUrl\": \"http://s-ings.com/typicons/\",\n  \"license\": \"CC BY-SA 3.0\",\n  \"licenseUrl\": \"https://creativecommons.org/licenses/by-sa/3.0/\"\n}, {\n  \"id\": \"go\",\n  \"name\": \"Github Octicons icons\",\n  \"projectUrl\": \"https://octicons.github.com/\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://github.com/primer/octicons/blob/master/LICENSE\"\n}, {\n  \"id\": \"fi\",\n  \"name\": \"Feather\",\n  \"projectUrl\": \"https://feathericons.com/\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://github.com/feathericons/feather/blob/master/LICENSE\"\n}, {\n  \"id\": \"lu\",\n  \"name\": \"Lucide\",\n  \"projectUrl\": \"https://lucide.dev/\",\n  \"license\": \"ISC\",\n  \"licenseUrl\": \"https://github.com/lucide-icons/lucide/blob/main/LICENSE\"\n}, {\n  \"id\": \"gi\",\n  \"name\": \"Game Icons\",\n  \"projectUrl\": \"https://game-icons.net/\",\n  \"license\": \"CC BY 3.0\",\n  \"licenseUrl\": \"https://creativecommons.org/licenses/by/3.0/\"\n}, {\n  \"id\": \"wi\",\n  \"name\": \"Weather Icons\",\n  \"projectUrl\": \"https://erikflowers.github.io/weather-icons/\",\n  \"license\": \"SIL OFL 1.1\",\n  \"licenseUrl\": \"http://scripts.sil.org/OFL\"\n}, {\n  \"id\": \"di\",\n  \"name\": \"Devicons\",\n  \"projectUrl\": \"https://vorillaz.github.io/devicons/\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://opensource.org/licenses/MIT\"\n}, {\n  \"id\": \"ai\",\n  \"name\": \"Ant Design Icons\",\n  \"projectUrl\": \"https://github.com/ant-design/ant-design-icons\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://opensource.org/licenses/MIT\"\n}, {\n  \"id\": \"bs\",\n  \"name\": \"Bootstrap Icons\",\n  \"projectUrl\": \"https://github.com/twbs/icons\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://opensource.org/licenses/MIT\"\n}, {\n  \"id\": \"ri\",\n  \"name\": \"Remix Icon\",\n  \"projectUrl\": \"https://github.com/Remix-Design/RemixIcon\",\n  \"license\": \"Apache License Version 2.0\",\n  \"licenseUrl\": \"http://www.apache.org/licenses/\"\n}, {\n  \"id\": \"fc\",\n  \"name\": \"Flat Color Icons\",\n  \"projectUrl\": \"https://github.com/icons8/flat-color-icons\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://opensource.org/licenses/MIT\"\n}, {\n  \"id\": \"gr\",\n  \"name\": \"Grommet-Icons\",\n  \"projectUrl\": \"https://github.com/grommet/grommet-icons\",\n  \"license\": \"Apache License Version 2.0\",\n  \"licenseUrl\": \"http://www.apache.org/licenses/\"\n}, {\n  \"id\": \"hi\",\n  \"name\": \"Heroicons\",\n  \"projectUrl\": \"https://github.com/tailwindlabs/heroicons\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://opensource.org/licenses/MIT\"\n}, {\n  \"id\": \"hi2\",\n  \"name\": \"Heroicons 2\",\n  \"projectUrl\": \"https://github.com/tailwindlabs/heroicons\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://opensource.org/licenses/MIT\"\n}, {\n  \"id\": \"si\",\n  \"name\": \"Simple Icons\",\n  \"projectUrl\": \"https://simpleicons.org/\",\n  \"license\": \"CC0 1.0 Universal\",\n  \"licenseUrl\": \"https://creativecommons.org/publicdomain/zero/1.0/\"\n}, {\n  \"id\": \"sl\",\n  \"name\": \"Simple Line Icons\",\n  \"projectUrl\": \"https://thesabbir.github.io/simple-line-icons/\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://opensource.org/licenses/MIT\"\n}, {\n  \"id\": \"im\",\n  \"name\": \"IcoMoon Free\",\n  \"projectUrl\": \"https://github.com/Keyamoon/IcoMoon-Free\",\n  \"license\": \"CC BY 4.0 License\",\n  \"licenseUrl\": \"https://github.com/Keyamoon/IcoMoon-Free/blob/master/License.txt\"\n}, {\n  \"id\": \"bi\",\n  \"name\": \"BoxIcons\",\n  \"projectUrl\": \"https://github.com/atisawd/boxicons\",\n  \"license\": \"CC BY 4.0 License\",\n  \"licenseUrl\": \"https://github.com/atisawd/boxicons/blob/master/LICENSE\"\n}, {\n  \"id\": \"cg\",\n  \"name\": \"css.gg\",\n  \"projectUrl\": \"https://github.com/astrit/css.gg\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://opensource.org/licenses/MIT\"\n}, {\n  \"id\": \"vsc\",\n  \"name\": \"VS Code Icons\",\n  \"projectUrl\": \"https://github.com/microsoft/vscode-codicons\",\n  \"license\": \"CC BY 4.0\",\n  \"licenseUrl\": \"https://creativecommons.org/licenses/by/4.0/\"\n}, {\n  \"id\": \"tb\",\n  \"name\": \"Tabler Icons\",\n  \"projectUrl\": \"https://github.com/tabler/tabler-icons\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://opensource.org/licenses/MIT\"\n}, {\n  \"id\": \"tfi\",\n  \"name\": \"Themify Icons\",\n  \"projectUrl\": \"https://github.com/lykmapipo/themify-icons\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://github.com/thecreation/standard-icons/blob/master/modules/themify-icons/LICENSE\"\n}, {\n  \"id\": \"rx\",\n  \"name\": \"Radix Icons\",\n  \"projectUrl\": \"https://icons.radix-ui.com\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://github.com/radix-ui/icons/blob/master/LICENSE\"\n}, {\n  \"id\": \"pi\",\n  \"name\": \"Phosphor Icons\",\n  \"projectUrl\": \"https://github.com/phosphor-icons/core\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://github.com/phosphor-icons/core/blob/main/LICENSE\"\n}, {\n  \"id\": \"lia\",\n  \"name\": \"Icons8 Line Awesome\",\n  \"projectUrl\": \"https://icons8.com/line-awesome\",\n  \"license\": \"MIT\",\n  \"licenseUrl\": \"https://github.com/icons8/line-awesome/blob/master/LICENSE.md\"\n}];","import React from \"react\";\nexport var DefaultContext = {\n  color: undefined,\n  size: undefined,\n  className: undefined,\n  style: undefined,\n  attr: undefined\n};\nexport var IconContext = React.createContext && React.createContext(DefaultContext);","var __assign = this && this.__assign || function () {\n  __assign = Object.assign || function (t) {\n    for (var s, i = 1, n = arguments.length; i < n; i++) {\n      s = arguments[i];\n\n      for (var p in s) {\n        if (Object.prototype.hasOwnProperty.call(s, p)) t[p] = s[p];\n      }\n    }\n\n    return t;\n  };\n\n  return __assign.apply(this, arguments);\n};\n\nvar __rest = this && this.__rest || function (s, e) {\n  var t = {};\n\n  for (var p in s) {\n    if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0) t[p] = s[p];\n  }\n\n  if (s != null && typeof Object.getOwnPropertySymbols === \"function\") for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) {\n    if (e.indexOf(p[i]) < 0 && Object.prototype.propertyIsEnumerable.call(s, p[i])) t[p[i]] = s[p[i]];\n  }\n  return t;\n};\n\nimport React from \"react\";\nimport { IconContext, DefaultContext } from \"./iconContext\";\n\nfunction Tree2Element(tree) {\n  return tree && tree.map(function (node, i) {\n    return React.createElement(node.tag, __assign({\n      key: i\n    }, node.attr), Tree2Element(node.child));\n  });\n}\n\nexport function GenIcon(data) {\n  // eslint-disable-next-line react/display-name\n  return function (props) {\n    return React.createElement(IconBase, __assign({\n      attr: __assign({}, data.attr)\n    }, props), Tree2Element(data.child));\n  };\n}\nexport function IconBase(props) {\n  var elem = function elem(conf) {\n    var attr = props.attr,\n        size = props.size,\n        title = props.title,\n        svgProps = __rest(props, [\"attr\", \"size\", \"title\"]);\n\n    var computedSize = size || conf.size || \"1em\";\n    var className;\n    if (conf.className) className = conf.className;\n    if (props.className) className = (className ? className + \" \" : \"\") + props.className;\n    return React.createElement(\"svg\", __assign({\n      stroke: \"currentColor\",\n      fill: \"currentColor\",\n      strokeWidth: \"0\"\n    }, conf.attr, attr, svgProps, {\n      className: className,\n      style: __assign(__assign({\n        color: props.color || conf.color\n      }, conf.style), props.style),\n      height: computedSize,\n      width: computedSize,\n      xmlns: \"http://www.w3.org/2000/svg\"\n    }), title && React.createElement(\"title\", null, title), props.children);\n  };\n\n  return IconContext !== undefined ? React.createElement(IconContext.Consumer, null, function (conf) {\n    return elem(conf);\n  }) : elem(DefaultContext);\n}","const getPublicationInfo = (types) => {\n    //static/miuiplus.webp\n    const pub_list = [\n        {\n            author: \"Xuechen Zhao, Lei Tian, Liqun Gao, <span style='font-weight:bold'}>Feng Xie</span>, Haiyang Wang, Hongzhou Wu, Bin Zhou.\",\n            title: 'MSFR: Stance Detection based on Multi-aspect Semantic Feature Representation via Hierarchical Contrastive Learning',\n            venues: '2024 IEEE International Conference on Acoustics, Speech and Signal Processing',\n            abbr: 'ICASSP 2024',\n            rank: 'CCF-B',\n            type: 'NLP',\n            pdf: '',\n            code: '',\n            page: '',\n            slide: '',\n            comment: '',\n            showmain: 'false',\n            tldr: ''\n        },\n        {\n            author: \"<span style='font-weight:bold'}>Feng Xie</span>, Xin Song, Xiang Zeng, Xuechen Zhao, Lei Tian, Bin Zhou, Yusong Tan.\",\n            title: 'MixTEA: Semi-supervised Entity Alignment with Mixture Teaching',\n            venues: 'The 2023 Conference on Empirical Methods in Natural Language Processing',\n            abbr: 'EMNLP 2023',\n            rank: 'CCF-B',\n            type: 'KG',\n            pdf: 'https://aclanthology.org/2023.findings-emnlp.63/',\n            code: 'https://github.com/Xiefeng69/MixTEA',\n            page: '',\n            slide: '',\n            comment: '',\n            showmain: 'true',\n            tldr: 'Semi-supervised entity alignment (EA) is a practical and challenging task because of the lack of adequate labeled mappings as training data. Most works address this problem by generating pseudo mappings for unlabeled entities. However, they either suffer from the erroneous (noisy) pseudo mappings or largely ignore the uncertainty of pseudo mappings. In this paper, we propose a novel semi-supervised EA method, termed as MixTEA, which guides the model learning with an end-to-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings. We firstly train a student model using few labeled mappings as standard. More importantly, in pseudo mapping learning, we propose a bi-directional voting (BDV) strategy that fuses the alignment decisions in different directions to estimate the uncertainty via the joint matching confidence score. Meanwhile, we also design a matching diversity-based rectification (MDR) module to adjust the pseudo mapping learning, thus reducing the negative influence of noisy mappings. Extensive results on benchmark datasets as well as further analyses demonstrate the superiority and the effectiveness of our proposed method.'\n        },\n        {\n            author: \"Haiyang Wang, Ye Wang, Xin Song, Bin Zhou, Xuechen Zhao, <span style='font-weight:bold'}>Feng Xie</span>.\",\n            title: 'Quantifying controversy from stance, sentiment, offensiveness and sarcasm: a fine-grained controversy intensity measurement framework on a Chinese dataset',\n            venues: 'World Wide Web Journal',\n            abbr: '',\n            rank: 'CCF-B',\n            type: 'NLP',\n            pdf: 'https://link.springer.com/article/10.1007/s11280-023-01191-x',\n            code: '',\n            page: '',\n            slide: '',\n            comment: '',\n            showmain: 'false',\n            tldr: 'Controversy measurement on social media plays an important part in understanding public opinion. Various topics are frequently hotly debated on social media platforms including Twitter and Sina Weibo. People sometimes use offensive or sarcastic language to convey their opinions about a topic or a source post, which might spark heated discussions and controversy towards related topics. Recent researches take controversy detection as a binary classification problem with two labels: controversy or non-controversy. The reason might be lacking a comprehensive understanding of why the controversy happened and a specific imagination of how it will be used in the downstream tasks. However, we believe that the degree of controversy courted by posts or topics in a real scenario varied. And fine-grained measurement of controversy will be beneficial to public sentiment identification, influence assessment and other social network analysis tasks. We also notice that the existing benchmarks of controversy detection are not applicable for fine-grained topic-level controversy measurement. In this paper, we present ProsCons, a large-scale comprehensive Chinese dataset that includes 245 topics and 32,667 posts with pro, con or neutral stances. Based on that, we design a controversy measurement framework for measuring the controversy intensity that topics sparked. This framework considers the degree of antagonism in terms of stance and sentiment, as well as the irrational degree (offensive or sarcasm) of a post to compute a controversy intensity. ProsCons provides a new benchmark for Chinese stance detection, offensive language and sarcasm detection, contributing to the multi-task learning of them. We conduct extensive experiments on ProsCons and provide baselines for these tasks. The experimental results highlight the challenges of the aforementioned tasks based on the ProsCons.'\n        },\n        {\n            author: \"Yuying Liao, Le Ma, Bin Zhou, Xuechen Zhao, <span style='font-weight:bold'}>Feng Xie</span>.\",\n            title: 'DraftFed: a Draft-based Personalized Federated Learning Approach for Heterogeneous Convolutional Neural Networks',\n            venues: 'IEEE Transactions on Mobile Computing',\n            abbr: '',\n            rank: 'CCF-A',\n            type: '',\n            pdf: 'https://doi.org/10.1109/TMC.2023.3283557',\n            code: '',\n            page: '',\n            slide: '',\n            comment: '',\n            showmain: 'false',\n            tldr: 'In conventional federated learning, each device is restricted to train a network model of a same structure. This greatly hinders the application of federated learning in edge devices and IoT scenarios where the data and devices are quite heterogeneous because of their different hardware equipment and communication networks. At the same time, most of the existing studies about federated learning of heterogeneous models are limited to horizontal heterogeneity which share a highly homogeneous vertical structure. Little work has been done on vertical heterogeneity such as models with different number of functional layers or different connection methods within them, not to mention the integrated heterogeneity scenarios. In DraftFed, a novel draft-based approach is proposed to implement personalized federated learning for integrated heterogeneous models. Unlike traditional federated learning in which the parameters/gradients are exchanged, DraftFed uses drafts as key knowledge to guide mutual learning of models, which makes it suitable for model structure personalization application scenarios.'\n        },\n        {\n            author: \"<span style='font-weight:bold'}>Feng Xie</span>, Xiang Zeng, Bin Zhou, Yusong Tan.\",\n            title: 'Improving Knowledge Graph Entity Alignment with Graph Augmentation',\n            venues: 'The 27th Pacific-Asia Conference on Knowledge Discovery and Data Mining',\n            abbr: 'PAKDD 2023',\n            rank: 'CCF-C',\n            type: 'KG',\n            pdf: 'https://link.springer.com/chapter/10.1007/978-3-031-33377-4_1',\n            code: 'https://github.com/Xiefeng69/GAEA',\n            page: '',\n            slide: '/static/GAEA_presentation.pdf',\n            comment: '',\n            showmain: 'true',\n            tldr: \"Entity alignment (EA) which links equivalent entities across different knowledge graphs (KGs) plays a crucial role in knowledge fusion. In recent years, graph neural networks (GNNs) have been successfully applied in many embedding-based EA methods. However, existing GNN-based methods either suffer from the structural heterogeneity issue that especially appears in the real KG distributions or ignore the heterogeneous representation learning for unseen (unlabeled) entities, which would lead the model to overfit on few alignment seeds (i.e., training data) and thus cause unsatisfactory alignment performance. To enhance the EA ability, we propose GAEA, a novel EA approach based on graph augmentation. In this model, we design a simple Entity-Relation (ER) Encoder to generate latent representations for entities via jointly modeling comprehensive structural information and rich relation semantics. Moreover, we use graph augmentation to create two graph views for margin-based alignment learning and contrastive entity representation learning, thus mitigating structural heterogeneity and further improving the model's alignment performance. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of our method.\"\n        },\n        {\n            author: \"<span style='font-weight:bold'}>Feng Xie</span>*, Zhong Zhang*, Xuechen Zhao, Haiyang Wang, Jiaying Zou, Lei Tian, Bin Zhou, Yusong Tan.\",\n            title: 'Adversarial Learning-based Stance Classifier for COVID-19-related Health Policies',\n            venues: 'The 28th International Conference on Database Systems for Advanced Applications',\n            abbr: 'DASFAA 2023',\n            rank: 'CCF-B',\n            type: 'NLP',\n            pdf: 'https://link.springer.com/chapter/10.1007/978-3-031-30678-5_18',\n            poster: '/static/dasfaa-poster.pdf',\n            code: 'https://github.com/Xiefeng69/stance-detection-for-covid19-related-health-policies',\n            page: '',\n            slide: '',\n            comment: '',\n            showmain: 'true',\n            tldr: \"The ongoing COVID-19 pandemic has caused immeasurable losses for people worldwide. To contain the spread of the virus and further alleviate the crisis, various health policies (e.g., stay-at-home orders) have been issued which spark heated discussions as users turn to share their attitudes on social media. In this paper, we consider a more realistic scenario on stance detection (i.e., cross-target and zero-shot settings) for the pandemic and propose an adversarial learning-based stance classifier to automatically identify the public's attitudes toward COVID-19-related health policies. Specifically, we adopt adversarial learning that allows the model to train on a large amount of labeled data and capture transferable knowledge from source topics, so as to enable generalize to the emerging health policies with sparse labeled data. To further enhance the model's deeper understanding, we incorporate policy descriptions as external knowledge into the model. Meanwhile, a GeoEncoder is designed which encourages the model to capture unobserved background factors specified by each region and then represent them as non-text information. We evaluate the performance of a broad range of baselines on the stance detection task for COVID-19-related health policies, and experimental results show that our proposed method achieves state-of-the-art performance in both cross-target and zero-shot settings.\",\n        },\n        {\n            author: \"Hayat Dino Bedru, Chen Zhang, <span style={{fontWeight:'bold'}}>Feng Xie</span>, Shuo Yu, Iftikhar Hussain.\",\n            title: 'CLARA: Citation and Similarity-based Author Ranking',\n            venues: 'Scientometrics',\n            abbr: '',\n            rank: 'JCR Q2',\n            type: 'TSA',\n            pdf: 'https://doi.org/10.1007/s11192-022-04590-5',\n            code: '',\n            page: '',\n            slide: '',\n            comment: 'This work was done when I was in DLUT',\n            showmain: 'false',\n            tldr: \"Scientific collaboration is getting tremendous attention from scholars and becoming the most common way of producing research works from different disciplines, enabling them to solve complex problems. Nevertheless, when the number of collaborators increases in research work, it becomes challenging to single out and recognize one scholar who contributes the most to the collaboration team of multiauthored publications. Hence, determining an influential author either from multiauthored papers or co-authorship networks is an interesting research problem. To address these problems, we develop a citation and similarity-based author ranking method, namely CLARA, that captures the influential author in multiauthored publications. The method considers attributes of publications such as citing papers and co-cited papers and similarity between publications. Firstly, the method computes the contribution of the co-authors in a given paper by employing fractional counting metrics. Secondly, it computes the contextual similarity between the given paper and its co-cited papers. Finally, the method ranks each co-author using the mathematically defined metric, called KeyScore, and discovers the “key” author among the co-authors of the given paper. We validate our method by extracting the papers of the “Chinese Outstanding Youth” winning researchers from the Microsoft Academic Graph dataset. The experimental results show that the CLARA method performs well in identifying key authors accurately and effectively, despite the position of the authors in the author list of their corresponding papers.\"\n        },\n        {\n            author: \"Xuechen Zhao, Jiaying Zou, Zhong Zhang, <span style='font-weight:bold'}>Feng Xie</span>, Bin Zhou, Lei Tian.\",\n            title: 'Feature Enhanced Zero-Shot Stance Detection via Contrastive Learning',\n            venues: 'SIAM International Conference on Data Mining',\n            abbr: 'SDM 2023',\n            rank: 'CCF-B',\n            type: 'NLP',\n            pdf: 'https://doi.org/10.1137/1.9781611977653.ch101',\n            code: '',\n            page: '',\n            slide: '',\n            comment: '',\n            showmain: 'false',\n            tldr: 'Zero-shot stance detection is challenging because it requires detecting the stance of previously unseen targets in the inference phase. The ability to learn transferable target-invariant features is critical for zero-shot stance detection. In this paper, we propose a stance detection approach that can efficiently adapt to unseen targets, the core of which is to capture target-invariant syntactic expression patterns as transferable knowledge. To be specific, we first augment the data by masking the topic words of sentences, and then feed the augmented data to an unsupervised contrastive learning module to capture transferable features. Besides, to fit a specific target, we encode the raw text as target-specific features. Finally, we adopt an attention mechanism, which combines syntactic expression patterns with target-specific features to obtain enhanced features for predicting previously unseen targets. Experiments demonstrate that our model outperforms baselines on four benchmark datasets.'\n        },\n        {\n            author: \"<span style='font-weight:bold'}>Feng Xie</span>, Zhong Zhang, Liang Li, Bin Zhou, Yusong Tan.\",\n            title: 'EpiGNN: Exploring Spatial Transmission with Graph Neural Network for Regional Epidemic Forecasting',\n            venues: 'The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases',\n            abbr: 'ECML-PKDD 2022',\n            rank: 'CCF-B',\n            type: 'TSA',\n            pdf: 'https://link.springer.com/chapter/10.1007/978-3-031-26422-1_29',\n            code: 'https://github.com/Xiefeng69/EpiGNN',\n            page: '/EpiGNN',\n            slide: '/static/EpiGNN_presentation.pdf',\n            comment: '',\n            showmain: 'true',\n            tldr: \"Epidemic forecasting is the key to effective control of epidemic transmission and helps the world mitigate the crisis that threatens public health. To better understand the transmission and evolution of epidemics, we propose EpiGNN, a graph neural network-based model for epidemic forecasting. Specifically, we design a transmission risk encoding module to characterize local and global spatial effects of regions in epidemic processes and incorporate them into the model. Meanwhile, we develop a Region-Aware Graph Learner (RAGL) that takes transmission risk, geographical dependencies, and temporal information into account to better explore spatial-temporal dependencies and makes regions aware of related regions' epidemic situations. The RAGL can also combine with external resources, such as human mobility, to further improve prediction performance. Comprehensive experiments on five real-world epidemic-related datasets (including influenza and COVID-19) demonstrate the effectiveness of our proposed method and show that EpiGNN outperforms state-of-the-art baselines by 9.48% in RMSE.\"\n        },\n        {\n            author: \"<span style='font-weight:bold'}>Feng Xie</span>, Zhong Zhang, Xuechen Zhao, Bin Zhou, Yusong Tan.\",\n            title: 'Inter- and Intra-Series Embeddings Fusion Network for Epidemiological Forecasting',\n            venues: 'The 34th International Conference on Software Engineering & Knowledge Engineering',\n            abbr: 'SEKE 2022',\n            rank: 'CCF-C',\n            type: 'TSA',\n            pdf: 'http://dx.doi.org/10.18293/SEKE2022-109',\n            code: 'https://github.com/Xiefeng69/SEFNet',\n            page: '/SEFNet',\n            slide: '',\n            comment: '',\n            showmain: 'true',\n            tldr: 'The accurate forecasting of infectious epidemic diseases is the key to effective control of the epidemic situation in a region. Most existing methods ignore potential dynamic dependencies between regions or the importance of temporal dependencies and inter-dependencies between regions for prediction. In this paper, we propose an Inter- and Intra-Series Embeddings Fusion Network (SEFNet) to improve epidemic prediction performance. SEFNet consists of two parallel modules, named Inter-Series Embedding Module and Intra-Series Embedding Module. In Inter-Series Embedding Module, a multi-scale unified convolution component called Region-Aware Convolution is proposed, which cooperates with self-attention to capture dynamic dependencies between time series obtained from multiple regions. The Intra-Series Embedding Module uses Long Short-Term Memory to capture temporal relationships within each time series. Subsequently, we learn the influence degree of two embeddings and fuse them with the parametric-matrix fusion method. To further improve the robustness, SEFNet also integrates a traditional autoregressive component in parallel with nonlinear neural networks. Experiments on four real-world epidemic-related datasets show SEFNet is effective and outperforms state-of-the-art baselines.'\n        },\n    ]\n    if (types == \"all\"){\n        return pub_list\n    }\n    let new_pub_list = []\n    for (const item of pub_list){\n        if(item.type == types){\n            new_pub_list.push(item)\n        }\n    }\n    \n    return new_pub_list\n}\n\nexport default getPublicationInfo;"],"names":["DefaultContext","color","undefined","size","className","style","attr","IconContext","__assign","Object","assign","t","s","i","n","arguments","length","p","prototype","hasOwnProperty","call","apply","this","__rest","e","indexOf","getOwnPropertySymbols","propertyIsEnumerable","Tree2Element","tree","map","node","tag","key","child","GenIcon","data","props","IconBase","elem","conf","title","svgProps","computedSize","stroke","fill","strokeWidth","height","width","xmlns","children","Consumer","types","pub_list","author","venues","abbr","rank","type","pdf","code","page","slide","comment","showmain","tldr","poster","new_pub_list","item","push"],"sourceRoot":""}